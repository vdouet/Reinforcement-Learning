# Planning and Learning with Tabular Methods

**Model-based**: Reinforcement learning methods that require a model of the 
environment, such as dynamic programming and heuristic search. Model-based 
methods rely on *planning* as their primary component.

**Model-free**: Methods that can be used without a model, such as Monte Carlo 
and temporal-difference methods. Model-free methods primarily rely on 
*learning*.

**Model of the environment**: Anything that an agent can use to predict how the
environment will respond to its actions.

## Models and Planning

Given a state and an action, a model produces a prediction of the resultant 
next state and next reward. Some models produce a description of all 
possibilities and their probabilities; these we call *distribution models*.
Other models produce just one of the possibilities, sampled according to the 
probabilities; these we call *sample models*.  
Distribution models are stronger than sample models in that they can always be 
used to produce samples. However, in many applications it is much easier to 
obtain sample models than distribution models.

Given a starting state and action:
+ A sample model produces a possible transition.
+ A distribution model generates all possible transitions weighted by their 
probabilities of occurring. 

Given a starting state and a policy:
+ A sample model could produce an entire episode.
+ A distribution model could generate all possible episodes and their 
probabilities.

In either case, we say the model is used to *simulate* the environment and
*produce* simulated experience.

*Planning* is used to refer to any computational process that takes a model as
input and produces or improves a policy for interacting with the modeled 
environment:

<p align="center">
<img
src="https://github.com/vdouet/Reinforcement-Learning/blob/master/02%20-%20Reinforcement%20Learning%20Specialization%20-%20Alberta%20University%20/Images/planning.png"
alt="Update rule" title="Update rule" width="278" height="42" />
</p>

In AI, two distinct approaches to planning:
+ *State-space planning* is viewed primarily as a search through the state 
space for an optimal policy or an optimal path to a goal. Actions cause 
transitions from state to state, and value functions are computed over states.
+ *Plan-space planning* is instead a search through the space of plans. 
Operators transform one plan into another, and value functions, if any, are 
defined over the space of plans. Plan-space planning includes evolutionary 
methods and “partial-order planning,” a common kind of planning in AO in which 
the ordering of steps is not completely determined at all stages of planning. 
Plan-space methods are difficult to apply efficiently to the stochastic 
sequential decision problems that are the focus in reinforcement learning.

All state-space planning methods share a common structure:
+ All state-space planning methods involve computing value functions as a key 
intermediate step toward improving the policy.
+ They compute value functions by updates or backup operations applied to 
simulated experience.

<p align="center">
<img
src="https://github.com/vdouet/Reinforcement-Learning/blob/master/02%20-%20Reinforcement%20Learning%20Specialization%20-%20Alberta%20University%20/Images/unifiedview.png"
alt="Update rule" title="Update rule" width="550" height="51" />
</p>

The heart of both learning and planning methods is the estimation of value 
functions by backing-up update operations. The difference is that whereas 
planning uses simulated experience generated by a model, learning methods use 
real experience generated by the environment. The common structure means that 
many ideas and algorithms can be transferred between planning and learning. In 
many cases a learning algorithm can be substituted for the key update step of a 
planning method. Learning methods require only experience as input, and in many 
cases they can be applied to simulated experience just as well as to real 
experience.

## Dyna: Integrated Planning, Acting, and Learning

