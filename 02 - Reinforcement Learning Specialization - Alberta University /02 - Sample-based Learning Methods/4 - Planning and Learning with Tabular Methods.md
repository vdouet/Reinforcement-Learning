# Planning and Learning with Tabular Methods

**Model-based**: Reinforcement learning methods that require a model of the 
environment, such as dynamic programming and heuristic search. Model-based 
methods rely on *planning* as their primary component.

**Model-free**: Methods that can be used without a model, such as Monte Carlo 
and temporal-difference methods. Model-free methods primarily rely on 
*learning*.

**Model of the environment**: Anything that an agent can use to predict how the
environment will respond to its actions.

## Models and Planning

Given a state and an action, a model produces a prediction of the resultant 
next state and next reward. Some models produce a description of all 
possibilities and their probabilities; these we call *distribution models*.
Other models produce just one of the possibilities, sampled according to the 
probabilities; these we call *sample models*.  
Distribution models are stronger than sample models in that they can always be 
used to produce samples. However, in many applications it is much easier to 
obtain sample models than distribution models.

Given a starting state and action:
+ A sample model produces a possible transition.
+ A distribution model generates all possible transitions weighted by their 
probabilities of occurring. 

Given a starting state and a policy:
+ A sample model could produce an entire episode.
+ A distribution model could generate all possible episodes and their 
probabilities.

In either case, we say the model is used to *simulate* the environment and
*produce* simulated experience.

*Planning* is used to refer to any computational process that takes a model as
input and produces or improves a policy for interacting with the modeled 
environment:

<p align="center">
<img
src="https://github.com/vdouet/Reinforcement-Learning/blob/master/02%20-%20Reinforcement%20Learning%20Specialization%20-%20Alberta%20University%20/Images/planning.png"
alt="Update rule" title="Update rule" width="278" height="42" />
</p>

In AI, two distinct approaches to planning:
+ *State-space planning* is viewed primarily as a search through the state 
space for an optimal policy or an optimal path to a goal. Actions cause 
transitions from state to state, and value functions are computed over states.
+ *Plan-space planning* is instead a search through the space of plans. 
Operators transform one plan into another, and value functions, if any, are 
defined over the space of plans. Plan-space planning includes evolutionary 
methods and “partial-order planning,” a common kind of planning in AO in which 
the ordering of steps is not completely determined at all stages of planning. 
Plan-space methods are difficult to apply efficiently to the stochastic 
sequential decision problems that are the focus in reinforcement learning.

All state-space planning methods share a common structure:
+ All state-space planning methods involve computing value functions as a key 
intermediate step toward improving the policy.
+ They compute value functions by updates or backup operations applied to 
simulated experience.

<p align="center">
<img
src="https://github.com/vdouet/Reinforcement-Learning/blob/master/02%20-%20Reinforcement%20Learning%20Specialization%20-%20Alberta%20University%20/Images/unifiedview.png"
alt="Update rule" title="Update rule" width="550" height="51" />
</p>

The heart of both learning and planning methods is the estimation of value 
functions by backing-up update operations. The difference is that whereas 
planning uses simulated experience generated by a model, learning methods use 
real experience generated by the environment. The common structure means that 
many ideas and algorithms can be transferred between planning and learning. In 
many cases a learning algorithm can be substituted for the key update step of a 
planning method. Learning methods require only experience as input, and in many 
cases they can be applied to simulated experience just as well as to real 
experience.

## Dyna: Integrated Planning, Acting, and Learning

Within a planning agent there are at least two roles for real experience:
+ *Model Learning*: can be used to improve the model (make it more accurately 
match the real environment).
+ *Direct Reinforcement Learning*: can be used to directly improve the value `
function and policy using RL methods.

The diagram below shows the possible relationship between experience, model,
values and policy:


<p align="center">
<img
src="https://github.com/vdouet/Reinforcement-Learning/blob/master/02%20-%20Reinforcement%20Learning%20Specialization%20-%20Alberta%20University%20/Images/dynarelationship.png"
alt="Update rule" title="Update rule" width="590" height="289" />
</p>

Each arrow shows a relationship of influence and presumed improvement.
Experience can indirectly improve value functions and policies, this is called
*indirect reinforcement learning* and that is what is involved in planning.

Both direct and indirect methods have advantages and disadvantages:
+ Indirect methods often make fuller use of a limited amount of experience and
thus achieve a better policy with fewer environmental interactions.
+ Direct methods are much simpler and are not affected by biases in the design
of the model.
+ Some have argued that indirect methods are always superior to direct ones,
while others have argued that direct methods are responsible for most human and
animal learning.

Dyna-Q includes all of the processes shown in the diagram above, planning, 
acting, model-learning, and direct RL, all occurring continually:
+ The planning method is the random-sample one-step tabular Q-planning method.
+ The direct RL method is one-step tabular Q-learning.
+ The model-learning method is also table-based and assumes the environment is
deterministic. After each transition *St, At -> Rt+1, St+1* the model records
in its table entry for *St, At* the prediction that *Rt+1, St+1* will
deterministically follow.

The overall architecture of Dyna agents, of which the Dyna-Q algorithm is one 
example, is shown below:

<p align="center">
<img
src="https://github.com/vdouet/Reinforcement-Learning/blob/master/02%20-%20Reinforcement%20Learning%20Specialization%20-%20Alberta%20University%20/Images/dynaarchitecture.png"
alt="Update rule" title="Update rule" width="446" height="321" />
</p>

In Dyna-Q the policy is built at the same time as the agent is learning from
the environment, thus it converge faster. See Dyna Maze p. 164 for example.

## When the Model Is Wrong

Models may be incorrect because the environment is stochastic and only a 
limited number of samples have been observed, or because the model was learned 
using function approximation that has generalized imperfectly, or simply 
because the environment has changed and its new behavior has not yet been 
observed. When the model is incorrect, the planning process is likely to 
compute a suboptimal policy.
Sometimes, the suboptimal policy computed by the planning quickly leads to the
discovery and correction of the modeling error. This tends to happen when the 
model is optimistic in the sense of predicting greater reward or better state 
transitions than are actually possible. The planned policy attempts to exploit 
these opportunities and in doing so discovers that they do not exist. 
Greater diffculties arise when the environment changes to become better than it
was before, and yet the formerly correct policy does not reveal the 
improvement. In these cases the modeling error may not be detected for a long 
time, if ever.

A Dyna-Q+ agent can be able to overcome this last problem. This agent keeps 
track for each state–action pair of how many time steps have elapsed since the
pair was last tried in a real interaction with the environment. The more time 
that has elapsed, the greater (we might presume) the chance that the dynamics 
of this pair has changed and that the model of it is incorrect. To encourage 
behavior that tests long-untried actions, a special “bonus reward” is given on 
simulated experiences involving these actions.


