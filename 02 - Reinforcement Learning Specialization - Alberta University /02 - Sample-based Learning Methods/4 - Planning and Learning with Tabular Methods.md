# Planning and Learning with Tabular Methods

**Model-based**: Reinforcement learning methods that require a model of the 
environment, such as dynamic programming and heuristic search. Model-based 
methods rely on *planning* as their primary component.

**Model-free**: Methods that can be used without a model, such as Monte Carlo 
and temporal-difference methods. Model-free methods primarily rely on 
*learning*.

**Model of the environment**: Anything that an agent can use to predict how the
environment will respond to its actions.

## Models and Planning

Given a state and an action, a model produces a prediction of the resultant 
next state and next reward. Some models produce a description of all 
possibilities and their probabilities; these we call *distribution models*.
Other models produce just one of the possibilities, sampled according to the 
probabilities; these we call *sample models*.  
Distribution models are stronger than sample models in that they can always be 
used to produce samples. However, in many applications it is much easier to 
obtain sample models than distribution models.

A distribution model consists of the probabilities of next states and rewards 
for possible actions. *Expected updates* require a distribution model because
they involve computing expectations over all the possible next states and 
rewards.

A sample model produces single transitions and rewards generated according to 
these probabilities. It is needed to simulate interacting with the environment 
during which sample updates can be used.

Given a starting state and action:
+ A sample model produces a possible transition.
+ A distribution model generates all possible transitions weighted by their 
probabilities of occurring. 

Given a starting state and a policy:
+ A sample model could produce an entire episode.
+ A distribution model could generate all possible episodes and their 
probabilities.

In either case, we say the model is used to *simulate* the environment and
*produce* simulated experience.

*Planning* is used to refer to any computational process that takes a model as
input and produces or improves a policy for interacting with the modeled 
environment:

<p align="center">
<img
src="https://github.com/vdouet/Reinforcement-Learning/blob/master/02%20-%20Reinforcement%20Learning%20Specialization%20-%20Alberta%20University%20/Images/planning.png"
alt="Update rule" title="Update rule" width="278" height="42" />
</p>

In AI, two distinct approaches to planning:
+ *State-space planning* is viewed primarily as a search through the state 
space for an optimal policy or an optimal path to a goal. Actions cause 
transitions from state to state, and value functions are computed over states.
+ *Plan-space planning* is instead a search through the space of plans. 
Operators transform one plan into another, and value functions, if any, are 
defined over the space of plans. Plan-space planning includes evolutionary 
methods and “partial-order planning,” a common kind of planning in AO in which 
the ordering of steps is not completely determined at all stages of planning. 
Plan-space methods are difficult to apply efficiently to the stochastic 
sequential decision problems that are the focus in reinforcement learning.

All state-space planning methods share a common structure:
+ All state-space planning methods involve computing value functions as a key 
intermediate step toward improving the policy.
+ They compute value functions by updates or backup operations applied to 
simulated experience.

<p align="center">
<img
src="https://github.com/vdouet/Reinforcement-Learning/blob/master/02%20-%20Reinforcement%20Learning%20Specialization%20-%20Alberta%20University%20/Images/unifiedview.png"
alt="Update rule" title="Update rule" width="550" height="51" />
</p>

The heart of both learning and planning methods is the estimation of value 
functions by backing-up update operations. The difference is that whereas 
planning uses simulated experience generated by a model, learning methods use 
real experience generated by the environment. The common structure means that 
many ideas and algorithms can be transferred between planning and learning. In 
many cases a learning algorithm can be substituted for the key update step of a 
planning method. Learning methods require only experience as input, and in many 
cases they can be applied to simulated experience just as well as to real 
experience.

## Dyna: Integrated Planning, Acting, and Learning

Within a planning agent there are at least two roles for real experience:
+ *Model Learning (indirect learning)*: can be used to improve the model 
(make it more accurately match the real environment).
+ *Direct Reinforcement Learning*: can be used to directly improve the value
function and policy using RL methods.

The diagram below shows the possible relationship between experience, model,
values and policy:


<p align="center">
<img
src="https://github.com/vdouet/Reinforcement-Learning/blob/master/02%20-%20Reinforcement%20Learning%20Specialization%20-%20Alberta%20University%20/Images/dynarelationship.png"
alt="Update rule" title="Update rule" width="281" height="221" />
</p>

Each arrow shows a relationship of influence and presumed improvement.
Experience can indirectly improve value functions and policies, this is called
*indirect reinforcement learning* and that is what is involved in planning.

Both direct and indirect methods have advantages and disadvantages:
+ Indirect methods often make fuller use of a limited amount of experience and
thus achieve a better policy with fewer environmental interactions.
+ Direct methods are much simpler and are not affected by biases in the design
of the model.
+ Some have argued that indirect methods are always superior to direct ones,
while others have argued that direct methods are responsible for most human and
animal learning.

Dyna-Q includes all of the processes shown in the diagram above, planning, 
acting, model-learning, and direct RL, all occurring continually:
+ The planning method is the random-sample one-step tabular Q-planning method.
+ The direct RL method is one-step tabular Q-learning.
+ The model-learning method is also table-based and assumes the environment is
deterministic. After each transition *St, At -> Rt+1, St+1* the model records
in its table entry for *St, At* the prediction that *Rt+1, St+1* will
deterministically follow.

The overall architecture of Dyna agents, of which the Dyna-Q algorithm is one 
example, is shown below:

<p align="center">
<img
src="https://github.com/vdouet/Reinforcement-Learning/blob/master/02%20-%20Reinforcement%20Learning%20Specialization%20-%20Alberta%20University%20/Images/dynaarchitecture.png"
alt="Update rule" title="Update rule" width="446" height="321" />
</p>

In Dyna-Q the policy is built at the same time as the agent is learning from
the environment, thus it converge faster. See Dyna Maze p. 164 for example.

## When the Model Is Wrong

Models may be incorrect because the environment is stochastic and only a 
limited number of samples have been observed, or because the model was learned 
using function approximation that has generalized imperfectly, or simply 
because the environment has changed and its new behavior has not yet been 
observed. When the model is incorrect, the planning process is likely to 
compute a suboptimal policy.
Sometimes, the suboptimal policy computed by the planning quickly leads to the
discovery and correction of the modeling error. This tends to happen when the 
model is optimistic in the sense of predicting greater reward or better state 
transitions than are actually possible. The planned policy attempts to exploit 
these opportunities and in doing so discovers that they do not exist. 
Greater diffculties arise when the environment changes to become better than it
was before, and yet the formerly correct policy does not reveal the 
improvement. In these cases the modeling error may not be detected for a long 
time, if ever.

A Dyna-Q+ agent can be able to overcome this last problem. This agent keeps 
track for each state–action pair of how many time steps have elapsed since the
pair was last tried in a real interaction with the environment. The more time 
that has elapsed, the greater (we might presume) the chance that the dynamics 
of this pair has changed and that the model of it is incorrect. To encourage 
behavior that tests long-untried actions, a special “bonus reward” is given on 
simulated experiences involving these actions.

## Prioritized Sweeping

In the previous Dyna agents, simulated transitions are started in state–action 
pairs selected uniformly at random from all previously experienced pairs. But 
a uniform selection is usually not the best; planning can be much more 
efficient if simulated transitions and updates are focused on particular 
state–action pairs. It is natural to prioritize the updates according to a 
measure of their urgency, and perform them in order of priority. This is the 
idea behind *prioritized sweeping.*  
Prioritized sweeping is just one way of distributing computations to improve 
planning efficiency, and probably not the best way. One of prioritized 
sweeping’s limitations is that it uses expected updates, which in stochastic 
environments may waste lots of computation on low-probability transitions

## Expected vs. Sample Updates

One-step updates vary primarily along three binary dimensions:
+ Whether they update state values or action values.
+ Whether they estimate the value for the optimal policy or for an arbitrary
given policy.
+ Whether the updates are *expected updates*, considering all possible events.
+ that might happen, or *sample updates*, considering a single sample of what 
might happen.

These three binary dimensions give rise to eight cases, seven of which 
correspond to specific algorithms, as shown below:

<p align="center">
<img
src="https://github.com/vdouet/Reinforcement-Learning/blob/master/02%20-%20Reinforcement%20Learning%20Specialization%20-%20Alberta%20University%20/Images/onestepupdate.png"
alt="Update rule" title="Update rule" width="358" height="582" />
</p>

Any of these one-step updates can be used in planning methods. For stochastic 
problems, prioritized sweeping is always done using one of the expected 
updates.

Sample updates:
+ \+ Can be done in an absence of a distribution model
+ \+ Require less computation
+ \- Sampling error

Expected updates:
+ \+ Better estimate because they are uncorrupted by sampling error
+ \- Cannot be done in an absence of a distribution model
+ \- Require more computation

For expected updates the computation required is usually dominated by the 
number of state–action pairs at which *Q* is evaluated. In large problems with 
many state–action pairs, we are often required to use sample updates. With so 
many state–action pairs, expected updates of all of them would take a very long
time.

Update from Drew Bagnell: Self-driving, robotics, and Model Based RL
+ All practical robot learning for decisions is model based
+ "It's better to die in simulation than in the real world"
+ Why? Sample complexity: It can take exponentially fewer interactions to learn
with a model than without ("Model-based RL in Contextual Decision Processes: PAC
bounds and Exponential Improvements over Model-free Approaches", Wen Sun et 
al., 2019).

## Trajectory Sampling

We compare two way of distributing updates.  
*Exhaustive sweeps*: The classical approach, from dynamic programming, is to 
perform sweeps through the entire state (or state–action) space, updating each 
state (or state–action pair) once per sweep.

*Trajectory sampling*: distributes updates according to the on-policy 
distribution (distribution observed when following the current policy), it 
simulates explicit individual trajectories and performs updates at the state or
state–action pairs encountered along the way.

Exhaustive sweeps:
+ Problematic on large tasks because there may not be time to complete even 
one sweep.
+ In many tasks the vast majority of the states are irrelevant because they are
visited only under very poor policies or with very low probability.

Trajectory sampling:
+ Distribution easily generated, simply interacts with the model, 
following the current policy.
+ Efficient way of distributing updates.

If trajectory sampling is done by sampling uniformly the state-action space, it
would suffer from some of the same problems as exhaustive sweeps.

In the short term, sampling according to the on-policy distribution helps by 
focusing on states that are near descendants of the start state. If there are 
many states and a small branching factor, this effect will be large and 
long-lasting. In the long run, focusing on the on-policy distribution may hurt 
because the commonly occurring states all already have their correct values. 
Sampling them is useless, whereas sampling other states may actually perform 
some useful work.  
These results are not conclusive because they are only for problems generated 
in a particular, random way, but they do suggest that sampling according to the
on-policy distribution can be a great advantage for large problems, in 
particular for problems in which a small subset of the state–action space is 
visited under the on-policy distribution.

## Real-time Dynamic Programming

*Real-time dynamic programming*, or RTDP, is an on-policy trajectory-sampling 
version of the value-iteration algorithm of dynamic programming. RTDP is an 
example of an *asynchronous* DP algorithm. Asynchronous DP algorithms are not 
organized in terms of systematic sweeps of the state set; they update state 
values in any order whatsoever, using whatever values of other states happen to
be available. In RTDP, the update order is dictated by the order states are 
visited in real or simulated trajectories.  
For certain types of tasks satisfying reasonable conditions, RTDP is guaranteed
to find a policy that is optimal on the relevant states (*optimal partial 
policy*) without visiting every state infinitely often, or even without 
visiting some states at all. Those tasks are examples of *stochastic optimal 
path problems*. Examples of this kind of task are minimum-time control tasks, 
where each time step required to reach a goal produces a reward of -1.  
RTDP strongly focus on subsets of the states that are relevant to the problem’s
objective. This focus becomes increasingly narrow as learning continues. 
Because the convergence theorem for RTDP applies to the simulations, we know 
that RTDP will eventually focus only on relevant states, i.e., on states making
up optimal paths.

## Planning at Decision Time

Planning can be used in at least two ways:
+ *Background Planning*: as seen with DP and Dyna, use planning to gradually 
improve a policy or value function on the basis of simulated experience 
obtained from a model (either a sample or a distribution model). Planning is 
not focussed on the current state.
+ *Decision-time Planning*: Using simulated experience to select an action for 
the current stateHere planning focuses on a particular state. More generally, 
planning used in this way can look much deeper than one-step-ahead and evaluate
action choices leading to many different predicted state and reward 
trajectories. 

In decision-time planning the values and policy are specific to the current 
state and the action choices available there, so much so that the values and 
policy created by the planning process are typically discarded after being used
to select the current action. Both planning can be mixed, focus planning on the
current state and store the results of planning.

Decision-time planning is most useful in applications in which fast responses 
are not required. If low latency action selection is the priority, it is 
generally better doing planning in the background to compute a policy that can
then be rapidly applied to each newly encountered state.

## Heuristic Search

The classical state-space planning methods in artificial intelligence are 
decision-time planning methods collectively known as *heuristic search*. In 
heuristic search, for each state encountered, a large tree of possible 
continuations is considered. The approximate value function is applied to the 
leaf nodes and then backed up toward the current state at the root. The backing
up stops at the state–action nodes for the current state. Once the backed-up 
values of these nodes are computed, the best of them is chosen as the current 
action, and then all backed-up values are discarded.

Our greedy, ε-greedy, and UCB action-selection methods are not unlike heuristic
search on a smaller scale. Heuristic search can be viewed as an extension of 
the idea of a greedy policy beyond a single step. The point of searching deeper
than one step is to obtain better action selections. If one has a perfect model
and an imperfect action-value function, then in fact deeper search will usually
yield better policies.

Much of the effectiveness of heuristic search is due to its search tree being
tightly focused on the states and actions that might immediately follow the 
current state.

## Rollout Algorithms

Rollout algorithms are decision-time planning algorithms based on Monte Carlo 
control applied to simulated trajectories that all begin at the current 
environment state. They estimate action values for a given policy by averaging 
the returns of many simulated trajectories that start with each possible action
and then follow the given policy. When the action-value estimates are 
considered to be accurate enough, the action having the highest estimated value
is executed, after which the process is carried out anew from the resulting 
next state. They produce Monte Carlo estimates of action values only for each 
current state and for a given policy usually called the *rollout policy*. As 
decision-time planning algorithms, rollout algorithms make immediate use of 
these action-value estimates, then discard them. The aim of a rollout algorithm
is to improve upon the rollout policy; not to find an optimal policy.

## Monte Carlo Tree Search

Monte Carlo Tree Search (MCTS) is a recent and strikingly successful example 
of decision-time planning. At its base, MCTS is a rollout algorithm but 
enhanced by the addition of a means for accumulating value estimates obtained 
from the Monte Carlo simulations in order to successively direct simulations 
toward more highly-rewarding trajectories. The core idea of MCTS is to 
successively focus multiple simulations starting at the current state by 
extending the initial portions of trajectories that have received high 
evaluations from earlier simulations.

The striking success of decision-time planning by MCTS has deeply influenced 
artificial intelligence, and many researchers are studying modifications and 
extensions of the basic procedure for use in both games and single-agent 
applications.

